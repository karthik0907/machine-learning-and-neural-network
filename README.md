Regularisation and Kernel Effects in Support Vector Machines
Overview

This repository contains the code and materials for a machine learning tutorial analysing how regularisation strength and kernel choice influence the geometric properties and generalisation behaviour of Support Vector Machines (SVMs). The work focuses on understanding margin maximisation, support vector structure, and decision boundary complexity rather than solely comparing predictive accuracy.

The tutorial combines theoretical discussion, visual analysis, and controlled experiments using synthetic data to illustrate how SVM design choices affect learning behaviour.

Repository Structure
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ svm_analysis.ipynb        # Jupyter notebook with all experiments and plots
â”‚
â”œâ”€â”€ figures/
â”‚   â””â”€â”€ *.png                     # Figures generated by the notebook (optional export)
â”‚
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ LICENSE                       # License for code usage
â””â”€â”€ tutorial.pdf / tutorial.html  # Written tutorial submission

Requirements

The experiments were conducted using Python 3.
Required packages:

numpy

matplotlib

pandas

scikit-learn

You can install dependencies using:

pip install numpy matplotlib pandas scikit-learn

How to Run the Code

Clone the repository:

git clone <repository-url>
cd <>


Open the Jupyter notebook:

jupyter notebook notebook/svm_analysis.ipynb


Run all cells in order.
All figures and result tables used in the tutorial will be generated automatically.

Dataset Description

A synthetic two-dimensional binary classification dataset is generated programmatically using scikit-learn. The dataset includes nonlinear structure and moderate noise to highlight the strengths and limitations of different SVM configurations. A stratified trainâ€“test split ensures balanced evaluation.

No external datasets are required.

Experiments Conducted

The notebook evaluates:

Linear SVMs with varying regularisation strength 
ğ¶
C

RBF kernel SVMs with different 
ğ¶
C and 
ğ›¾
Î³ values

Metrics analysed include:

Training and test accuracy

Generalisation gap

Number of support vectors (kernel SVMs)

Margin estimates (linear SVMs)

Decision boundary visualisations

Accessibility Considerations

All plots use high-contrast, colour-blind-friendly palettes.

Figures are interpretable without relying on colour alone.

Clear headings and captions are provided for screen readers.

Tables present numeric values directly rather than through visual encoding.

License

This project is released under the MIT License
